{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print pyspark version and module location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.3.2\n",
      "Module location: /home/popoyi/ssdbox/fact/utilhub/spark-home/spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Module location: {pyspark.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializes and returns a SparkSession configured for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 11:00:21 WARN Utils: Your hostname, vyaakar-labs01 resolves to a loopback address: 127.0.1.1; using 192.168.0.104 instead (on interface wlp7s0)\n",
      "24/03/04 11:00:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 11:00:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/04 11:00:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"FHV Trip Data Analysis\") \\\n",
    "        .master(\"local[6]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the filepath to the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./assign01-fhv-ds/raw/fhv_tripdata_2019-10.csv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV file into a DataFrame using inferSchema to infer data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the first 10 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1897493\n",
      "Number of Columns: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of rows and columns\n",
    "num_rows = df.count()\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- dropoff_time: timestamp (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- special_request_flag: string (nullable = true)\n",
      " |-- affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "df_renamed = df.withColumnRenamed(\"dispatching_base_num\", \"dispatching_base_number\") \\\n",
    "               .withColumnRenamed(\"pickup_datetime\", \"pickup_time\") \\\n",
    "               .withColumnRenamed(\"dropOff_datetime\", \"dropoff_time\") \\\n",
    "               .withColumnRenamed(\"PUlocationID\", \"pickup_location_id\") \\\n",
    "               .withColumnRenamed(\"DOlocationID\", \"dropoff_location_id\") \\\n",
    "               .withColumnRenamed(\"SR_Flag\", \"special_request_flag\") \\\n",
    "               .withColumnRenamed(\"Affiliated_base_number\", \"affiliated_base_number\")\n",
    "\n",
    "# Show the schema of the DataFrame after renaming the columns to confirm the changes\n",
    "df_renamed.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Repartition the DataFrame to 6 partitions\n",
    "df_repartitioned = df.repartition(6)\n",
    "\n",
    "# Save the repartitioned DataFrame to a Parquet file\n",
    "parquet_path = \"./assign01-fhv-ds/pq\"\n",
    "df_repartitioned.write.mode(\"overwrite\").parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02803|2019-10-24 14:00:26|2019-10-24 14:19:29|         242|         254|   null|                B02803|\n",
      "|              B01145|2019-10-12 10:32:20|2019-10-12 10:36:19|         264|         213|   null|                B02971|\n",
      "|              B01779|2019-10-04 10:30:00|2019-10-04 10:51:00|         264|         264|   null|                B01779|\n",
      "|              B02968|2019-10-05 05:47:36|2019-10-05 05:50:32|         264|         213|   null|                B02968|\n",
      "|     B01215         |2019-10-30 07:25:23|2019-10-30 07:37:22|           7|         223|   null|       B01215         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrame from a Parquet file\n",
    "df_from_parquet = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Print the schema of the loaded DataFrame\n",
    "df_from_parquet.printSchema()\n",
    "df_from_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- dropoff_time: timestamp (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- special_request_flag: string (nullable = true)\n",
      " |-- affiliated_base_number: string (nullable = true)\n",
      "\n",
      "+-----------------------+-------------------+-------------------+------------------+-------------------+--------------------+----------------------+\n",
      "|dispatching_base_number|        pickup_time|       dropoff_time|pickup_location_id|dropoff_location_id|special_request_flag|affiliated_base_number|\n",
      "+-----------------------+-------------------+-------------------+------------------+-------------------+--------------------+----------------------+\n",
      "|                 B02803|2019-10-24 14:00:26|2019-10-24 14:19:29|               242|                254|                null|                B02803|\n",
      "|                 B01145|2019-10-12 10:32:20|2019-10-12 10:36:19|               264|                213|                null|                B02971|\n",
      "|                 B01779|2019-10-04 10:30:00|2019-10-04 10:51:00|               264|                264|                null|                B01779|\n",
      "|                 B02968|2019-10-05 05:47:36|2019-10-05 05:50:32|               264|                213|                null|                B02968|\n",
      "|        B01215         |2019-10-30 07:25:23|2019-10-30 07:37:22|                 7|                223|                null|       B01215         |\n",
      "+-----------------------+-------------------+-------------------+------------------+-------------------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "df_from_parquet = df_from_parquet.withColumnRenamed(\"dispatching_base_num\", \"dispatching_base_number\") \\\n",
    "               .withColumnRenamed(\"pickup_datetime\", \"pickup_time\") \\\n",
    "               .withColumnRenamed(\"dropOff_datetime\", \"dropoff_time\") \\\n",
    "               .withColumnRenamed(\"PUlocationID\", \"pickup_location_id\") \\\n",
    "               .withColumnRenamed(\"DOlocationID\", \"dropoff_location_id\") \\\n",
    "               .withColumnRenamed(\"SR_Flag\", \"special_request_flag\") \\\n",
    "               .withColumnRenamed(\"Affiliated_base_number\", \"affiliated_base_number\")\n",
    "\n",
    "# Show the schema of the DataFrame after renaming the columns to confirm the changes\n",
    "df_from_parquet.printSchema()\n",
    "df_from_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records on 15th of October: 62610\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth, month, col\n",
    "\n",
    "filtered_df = df_from_parquet.filter(\n",
    "    (dayofmonth(col(\"pickup_time\")) == 15) & \n",
    "    (month(col(\"pickup_time\")) == 10)\n",
    ")\n",
    "\n",
    "record_count = filtered_df.count()\n",
    "print(f\"Number of records on 15th of October: {record_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+------------------+\n",
      "|year|month|day|longest_trip_hours|\n",
      "+----+-----+---+------------------+\n",
      "|2019|   10| 11|          631152.5|\n",
      "|2019|   10| 28|          631152.5|\n",
      "|2019|   10| 31| 87672.44083333333|\n",
      "|2019|   10|  1| 70128.02805555555|\n",
      "|2019|   10| 17|            8794.0|\n",
      "|2019|   10| 26| 8784.166666666666|\n",
      "|2019|   10| 30|1464.5344444444445|\n",
      "|2019|   10| 25|1056.8266666666666|\n",
      "|2019|   10|  2| 769.2313888888889|\n",
      "|2019|   10| 23| 745.6166666666667|\n",
      "+----+-----+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view\n",
    "df_from_parquet.createOrReplaceTempView(\"trips\")\n",
    "\n",
    "# Execute SQL Query to find the longest trip duration for each day\n",
    "longest_trip_query = \"\"\"\n",
    "SELECT \n",
    "    YEAR(pickup_time) AS year, \n",
    "    MONTH(pickup_time) AS month, \n",
    "    DAYOFMONTH(pickup_time) AS day, \n",
    "    MAX((UNIX_TIMESTAMP(dropoff_time) - UNIX_TIMESTAMP(pickup_time)) / 3600) AS longest_trip_hours\n",
    "FROM \n",
    "    trips\n",
    "GROUP BY \n",
    "    YEAR(pickup_time), \n",
    "    MONTH(pickup_time), \n",
    "    DAYOFMONTH(pickup_time)\n",
    "ORDER BY \n",
    "    longest_trip_hours\n",
    "DESC\n",
    "\"\"\"\n",
    "\n",
    "# Display the Results\n",
    "longest_trip_per_day = spark.sql(longest_trip_query)\n",
    "longest_trip_per_day.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_file_path = \"./assign01-fhv-ds/raw/taxi_zone_lookup.csv\"\n",
    "\n",
    "taxi_zone_df = spark.read.csv(taxi_zone_file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "taxi_zone_df = taxi_zone_df.withColumnRenamed(\"LocationID\", \"location_id\") \\\n",
    "               .withColumnRenamed(\"Borough\", \"borough\") \\\n",
    "               .withColumnRenamed(\"Zone\", \"zone\") \\\n",
    "               .withColumnRenamed(\"service_zone\", \"service_zone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+--------------------+------------+\n",
      "|location_id|      borough|                zone|service_zone|\n",
      "+-----------+-------------+--------------------+------------+\n",
      "|          1|          EWR|      Newark Airport|         EWR|\n",
      "|          2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|          3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|          4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|          5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+-----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_zone_df.printSchema()\n",
    "\n",
    "taxi_zone_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Temporary Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.createOrReplaceTempView(\"fhv_trips\")\n",
    "taxi_zone_df.createOrReplaceTempView(\"taxi_zones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_query = \"\"\"\n",
    "SELECT \n",
    "    t.zone AS pickup_zone_name,\n",
    "    COUNT(*) AS trip_count\n",
    "FROM \n",
    "    fhv_trips f\n",
    "JOIN \n",
    "    taxi_zones t ON f.pickup_location_id = t.location_id\n",
    "GROUP BY \n",
    "    pickup_zone_name\n",
    "\"\"\"\n",
    "\n",
    "# Creating a new DataFrame by executing the join query\n",
    "fhv_trip_and_zone_df = spark.sql(join_query)\n",
    "\n",
    "# Persist the joined DataFrame as a temporary view for further analysis\n",
    "fhv_trip_and_zone_df.createOrReplaceTempView(\"fhv_trip_and_zone_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_zone_name: string (nullable = true)\n",
      " |-- trip_count: long (nullable = false)\n",
      "\n",
      "+--------------------+----------+\n",
      "|    pickup_zone_name|trip_count|\n",
      "+--------------------+----------+\n",
      "|           Homecrest|      1295|\n",
      "|              Corona|      7175|\n",
      "|    Bensonhurst West|      1880|\n",
      "|         Westerleigh|      1317|\n",
      "|Charleston/Totten...|      2533|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fhv_trip_and_zone_df.printSchema()\n",
    "\n",
    "fhv_trip_and_zone_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find Least Frequent Pickup Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|    pickup_zone_name|trip_count|\n",
      "+--------------------+----------+\n",
      "|         Jamaica Bay|         1|\n",
      "|Governor's Island...|         2|\n",
      "| Green-Wood Cemetery|         5|\n",
      "|       Broad Channel|         8|\n",
      "|     Highbridge Park|        14|\n",
      "|        Battery Park|        15|\n",
      "|Saint Michaels Ce...|        23|\n",
      "|Breezy Point/Fort...|        25|\n",
      "|Marine Park/Floyd...|        26|\n",
      "|        Astoria Park|        29|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "least_frequent_query = \"\"\"\n",
    "SELECT \n",
    "    pickup_zone_name,\n",
    "    trip_count\n",
    "FROM \n",
    "    fhv_trip_and_zone_df\n",
    "ORDER BY \n",
    "    trip_count ASC\n",
    "\"\"\"\n",
    "\n",
    "least_frequent_pickup_zone = spark.sql(least_frequent_query)\n",
    "least_frequent_pickup_zone.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop the SparkSession when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "004-06-pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
